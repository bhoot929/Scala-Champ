package com.myCompany.scala

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import org.apache.hadoop.fs

/* you have to set below dependency inorder to read data using S3a
<dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-aws</artifactId>
          <version>2.9.0</version>
      </dependency>
      <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-common</artifactId>
          <version>2.9.0</version>
      </dependency>
      <dependency>
          <groupId>com.amazonaws</groupId>
          <artifactId>aws-java-sdk</artifactId>
          <version>1.11.344</version> */


object ReadS3Data {

  /** Our main function where the action happens */
  def main(args: Array[String]) {

    // Set the log level to only print errors
    Logger.getLogger("org").setLevel(Level.ERROR)
    val conf = new SparkConf().setAppName("ReadS3Data").setMaster("local")
    val sc = new SparkContext(conf)
    sc.hadoopConfiguration.set("fs.s3a.impl","org.apache.hadoop.fs.s3a.S3AFileSystem")
    sc.hadoopConfiguration.set("fs.s3a.access.key", accesskey)
    sc.hadoopConfiguration.set("fs.s3a.secret.key",secretkey)
    //sc.hadoopConfiguration.set("fs.s3a.fast.upload", "true")
    // Create a SparkContext using every core of the local machine
    //val sc = new SparkContext("local[*]", "WordCount")

    // Read each line of my book into an RDD
    val p = sc.textFile("s3a://bucketname/filename")

    val words = p.flatMap(x => x.split(" "))

    val wordcount = words.countByValue()

    wordcount.foreach(println)


  }

}

